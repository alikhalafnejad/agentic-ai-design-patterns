## **فصل ۷: الگوی حافظه (The Memory Pattern)**

اگر در زندگی واقعی کسی هیچ‌چیز از گذشته خود را به یاد نمی‌آورد، تصمیم‌های هوشمندانه‌ای نمی‌تواند بگیرد. همین اصل ساده در دنیای عامل‌های هوش مصنوعی نیز برقرار است.  
یک عامل که فقط ورودی فعلی کاربر را تحلیل می‌کند، بدون آگاهی از مکالمات و تجربیات گذشته، در واقع نوعی پاسخ‌دهنده آماری است نه یک موجود «هوشمند».  
برای اینکه عاملی واقعاً بتواند یاد بگیرد، سازگار شود و تجربه انباشته بسازد، باید حافظه داشته باشد.

---

### **مشکل (Problem)**

مدل‌های زبانی بزرگ (LLMs) ذاتاً حالت‌ناپذیر (stateless) هستند — یعنی پس از پایان یک تعامل، هیچ‌چیز از محتوای قبلی را نگه نمی‌دارند. مگر آن‌که توسعه‌دهنده به‌صورت دستی خلاصه‌ای از مکالمه یا داده‌های محتوایی را دوباره در پرامپت وارد کند.

این رویکرد موقتی هرچند کاربردی است، اما دارای محدودیت‌های جدی است:

*   حافظه کوتاه‌مدت مدل معمولاً با محدودیت توکن (Token Limit) در Input Context مواجه است.
*   ذخیره کامل تاریخچه مکالمه باعث افزایش چشمگیر هزینه و تأخیر در واکنش می‌شود.
*   بدون سازوکار انتخابی و فشرده‌سازی، داده‌های گذشته سریعاً غیرقابل مدیریت می‌شوند.

این چالش ما را به سمت طراحی ساختار حافظه‌ای می‌برد که هم کارآمد و هم هوشمند باشد.

---

### **راه‌حل (Solution)**

الگوی حافظه، مدل حافظه انسان را به‌صورت استعاری در معماری عامل بازسازی می‌کند.  
این الگو به عامل امکان می‌دهد اطلاعات مهم از تعاملات گذشته را ذخیره و هنگام تصمیم‌گیری‌های جدید، آن‌ها را بازیابی کند.

دو نوع حافظه اصلی در عامل‌ها وجود دارد:

1.  **حافظه کوتاه‌مدت (Short-Term Memory)**  
    – برای نگهداری وضعیت فعلی یا چند تعامل اخیر.  
    – معمولاً درون «context window» مدل باقی می‌ماند.  
    – مثال: حفظ جریان گفتگو در یک چت چندمرحله‌ای.

2.  **حافظه بلندمدت (Long-Term Memory)**  
    – برای نگهداری مؤثر اطلاعات گذشته، دانش مرتبط، و تجربه‌های پیشین عامل.  
    – معمولاً با استفاده از پایگاه داده برداری (Vector Database) مثل **FAISS، Milvus، Pinecone، یا chroma** پیاده‌سازی می‌شود.  
    – اطلاع مربوط به تعاملات گذشته به embedding (بردار معنایی) تبدیل و در این پایگاه داده ذخیره می‌شود تا در مواقع نیاز به صورت جستجوی معنایی (semantic search) بازیابی شود.

---

### **نحوه کارکرد (How It Works)**

فرآیند کلی حافظه در الگوی Agentic Design به صورت چرخه‌ای انجام می‌شود:

> **مشاهده (Observe)** → **ذخیره (Store)** → **جستجو (Retrieve)** → **استفاده (Use)**

1.  **ذخیره‌سازی (Storing):**  
    هر تعامل، خروجی یا دانش جدیدی که عامل تولید می‌کند، به یک بردار (embedding vector) تبدیل شده و همراه با متاداده (مثل زمان، منبع، نوع داده، یا وضعیت عامل) در پایگاه ذخیره می‌شود.

2.  **بازیابی (Retrieval):**  
    هنگام مواجهه با وظیفه‌ای جدید، عامل ابتدا جستجوی معنایی در حافظه بلندمدت انجام می‌دهد تا اطلاعات مرتبط با پرسش فعلی را بازیابی کند.

3.  **ادغام با پرامپت (Context Integration):**  
    داده‌های به‌دست‌آمده از حافظه با ورودی کاربر یا هدف فعلی ترکیب و به مدل داده می‌شوند تا تصمیم‌گیری آگاهانه‌تری انجام شود.

4.  **به‌روزرسانی و فراموشی (Updating & Forgetting):**  
    حافظه «زنده» باید قادر باشد داده‌های تکراری، بی‌اهمیت یا منسوخ شده را حذف کند تا فضای مفید حفظ شود.

---

### **نمونه معماری حافظه (Memory Architecture Example)**

```python
class MemoryManager:
    def __init__(self, vector_db):
        self.vector_db = vector_db

    def store(self, text, metadata):
        embedding = embed(text)  # Convert to vector
        self.vector_db.add(embedding, metadata)

    def retrieve(self, query, top_k=3):
        q_vec = embed(query)
        results = self.vector_db.search(q_vec, top_k)
        return [r.text for r in results]
```

سپس در حلقه عامل (Agent Loop):

```python
context = memory.retrieve(user_query)
response = llm.generate(prompt=user_query, context=context)
```

به این ترتیب، عامل می‌تواند از تجربه‌های قبلی خود بهره ببرد.

---

### **نکات طراحی (Design Principles)**

1.  **انتخاب‌ هوشمند (Selective Storage):**  
    همه داده‌ها ارزش نگهداری ندارند. باید فقط اسناد یا تعاملاتی ذخیره شوند که ارزش یادآوری دارند.

2.  **خلاصه‌سازی حافظه (Memory Summarization):**  
    مثلاً با استفاده از مدل زبانی، هر شب عامل بتواند تعاملات روز را خلاصه و در قالب دانش فشرده ذخیره کند.

3.  **دوگانگی حافظه (Dual Memory Model):**  
    ترکیب حافظه کوتاه‌مدت داخل مدل و حافظه بلندمدت خارجی در پایگاه داده برداری.

4.  **حافظه یادگیرنده (Learning Memory):**  
    عامل می‌تواند از بازتاب‌ها و نتایج قبلی، قواعد جدید یا insightهای استخراج‌شده را ذخیره کند تا در مأموریت‌های آینده استفاده کند.

---

### **مزایا (Benefits)**

*   **یادگیری مداوم:** عامل می‌تواند تجربه‌ها را نگه دارد و هوشمندانه‌تر تصمیم بگیرد.  
*   **استمرار گفتگو:** در تعاملات طولانی‌مدت، عامل می‌فهمد چه گفته شده و چه تصمیمی گرفته شده.  
*   **کاهش خطاهای تکراری:** از اشتباهات گذشته درس می‌گیرد.  
*   **با زمینه‌تر شدن پاسخ‌ها:** پاسخ‌ها متکی بر حافظه تعاملی هیستوریک هستند نه فقط ورودی فعلی.

---

### **ملاحظات و چالش‌ها (Challenges and Considerations)**

*   **مقیاس‌پذیری داده:** حافظه بلندمدت می‌تواند به سرعت حجیم شود؛ نیاز به index و pruning دارد.  
*   **حریم خصوصی:** داده‌های ذخیره شده توسط عامل ممکن است شامل اطلاعات حساس کاربر باشند. هر حافظه باید محافظت و رمزگذاری شود.  
*   **تطبیق حافظه با مدل:** اگر embeddingها از نسخه قدیمی مدل ساخته شده باشند، پس از آپدیت مدل ممکن است ناسازگار شوند؛ بنابراین نیاز به بازتولید بردارها است.  
*   **فراموشی هدفمند:** گاهی حذف اطلاعات منسوخ یا نادرست ضروری است تا عامل از بینش‌های اشتباه تغذیه نکند.

---

### **ترکیب با سایر الگوها (Pattern Integration)**

الگوی حافظه در هماهنگی کامل با دیگر الگوها عمل می‌کند:

*   با **Planning Pattern:** برای یادآوری برنامه‌های قبلی و درسی که از اجرای آن‌ها گرفته شده است.  
*   با **Reflection Pattern:** عامل نتایج بازتاب قبلی خود را ذخیره می‌کند تا کیفیت پاسخ آینده را افزایش دهد.  
*   با **Tool Use Pattern:** نتایج ابزارهایی مانند جستجو یا تحلیل داده‌ها در حافظه ضبط می‌شوند تا در آینده قابل رجوع باشند.

---

### **جمع‌بندی فصل**

الگوی حافظه، عنصر حیاتی هوش در هر عامل است. بدون آن، عامل صرفاً یک مدل تولید متن است؛ با آن، عامل تبدیل به موجودی دارای تجربه، تاریخچه و شخصیت می‌شود.  
در حقیقت، حافظه همان «نخ تداومی» است که بین جلسات، تصمیمات و تجربیات یک عامل، انسجام ایجاد می‌کند.  
عامل بدون حافظه، فقط پاسخ می‌دهد؛ عامل دارای حافظه، *یاد می‌گیرد.*

---

در فصل بعد، کتاب وارد یکی از پیشرفته‌ترین محورهای طراحی عامل‌ها می‌شود:  
**الگوی یادگیری و انطباق (The Learning and Adaptation Pattern)** — جایی که عامل نه‌تنها از گذشته یاد می‌گیرد، بلکه قواعد خود را تغییر می‌دهد تا برای آینده بهتر آماده شود.